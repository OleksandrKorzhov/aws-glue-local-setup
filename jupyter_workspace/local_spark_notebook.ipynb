{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-22T15:01:33.982584Z",
     "end_time": "2023-04-22T15:01:33.983126Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import one_file_lib\n",
    "from library import something\n",
    "\n",
    "# Importing libraries from outside of the jupyter_workspace does not work\n",
    "# from outer_library import something as outer_something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'Test!'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_file_lib.test()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T15:01:35.529816Z",
     "end_time": "2023-04-22T15:01:35.541868Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "'Something is done!'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "something.do_something()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T15:01:36.538771Z",
     "end_time": "2023-04-22T15:01:36.551091Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T20:44:39.978878Z",
     "end_time": "2023-04-21T20:44:39.986122Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['/home/glue_user/workspace/jupyter_workspace',\n 'total 80',\n 'drwxr-xr-x 10 glue_user root   320 Apr 22 17:31 .',\n 'drwxr-xr-x 17 glue_user root   544 Apr 22 17:31 ..',\n '-rw-r--r--  1 glue_user root 43179 Apr 21 14:03 glue_job_sample.ipynb',\n 'drwxr-xr-x  5 glue_user root   160 Apr 21 14:24 .ipynb_checkpoints',\n '-rw-r--r--  1 glue_user root  6129 Apr 20 17:29 jupyter_env.ipynb',\n 'drwxr-xr-x  5 glue_user root   160 Apr 22 17:30 library',\n '-rw-r--r--  1 root      root 20031 Apr 22 17:31 local_spark_notebook.ipynb',\n '-rw-r--r--  1 glue_user root    32 Apr 22 17:28 one_file_lib.py',\n 'drwxr-xr-x  3 glue_user root    96 Apr 22 17:29 __pycache__',\n '-rw-r--r--  1 glue_user root  3413 Apr 21 14:34 spark_job_sample.ipynb',\n 'Package              Version',\n '-------------------- -----------',\n 'aiobotocore          1.4.2',\n 'aiohttp              3.8.3',\n 'aioitertools         0.11.0',\n 'aiosignal            1.3.1',\n 'anyio                3.6.2',\n 'argon2-cffi          21.3.0',\n 'argon2-cffi-bindings 21.2.0',\n 'async-timeout        4.0.2',\n 'asynctest            0.13.0',\n 'attrs                22.2.0',\n 'autovizwidget        0.20.3',\n 'avro-python3         1.10.2',\n 'Babel                2.11.0',\n 'backcall             0.2.0',\n 'beautifulsoup4       4.11.1',\n 'bleach               5.0.1',\n 'boto                 2.49.0',\n 'boto3                1.18.50',\n 'botocore             1.20.106',\n 'certifi              2021.5.30',\n 'cffi                 1.15.1',\n 'chardet              3.0.4',\n 'charset-normalizer   2.1.1',\n 'click                8.1.3',\n 'cryptography         39.0.0',\n 'cycler               0.10.0',\n 'Cython               0.29.4',\n 'decorator            5.1.1',\n 'defusedxml           0.7.1',\n 'deprecation          2.1.0',\n 'docutils             0.17.1',\n 'entrypoints          0.4',\n 'enum34               1.1.10',\n 'fastjsonschema       2.16.2',\n 'frozenlist           1.3.3',\n 'fsspec               2021.8.1',\n 'gssapi               1.8.2',\n 'hdijupyterutils      0.20.3',\n 'idna                 2.10',\n 'importlib-metadata   6.0.0',\n 'importlib-resources  5.10.2',\n 'iniconfig            2.0.0',\n 'ipykernel            5.5.3',\n 'ipython              7.34.0',\n 'ipython-genutils     0.2.0',\n 'ipywidgets           7.6.5',\n 'jedi                 0.18.2',\n 'Jinja2               3.1.2',\n 'jmespath             0.10.0',\n 'joblib               1.0.1',\n 'json5                0.9.11',\n 'jsonschema           4.17.3',\n 'jupyter              1.0.0',\n 'jupyter_client       7.4.8',\n 'jupyter-console      6.4.4',\n 'jupyter_core         4.12.0',\n 'jupyter_packaging    0.12.3',\n 'jupyter-server       1.23.4',\n 'jupyterlab           3.0.14',\n 'jupyterlab-pygments  0.2.2',\n 'jupyterlab_server    2.18.0',\n 'jupyterlab-widgets   1.1.1',\n 'kiwisolver           1.3.2',\n 'krb5                 0.4.1',\n 'MarkupSafe           2.1.1',\n 'matplotlib           3.4.3',\n 'matplotlib-inline    0.1.6',\n 'mistune              2.0.4',\n 'mock                 5.0.1',\n 'mpmath               1.2.1',\n 'multidict            6.0.4',\n 'nbclassic            0.4.8',\n 'nbclient             0.7.2',\n 'nbconvert            7.2.7',\n 'nbformat             5.7.1',\n 'nest-asyncio         1.5.6',\n 'nltk                 3.6.3',\n 'nose                 1.3.7',\n 'notebook             6.5.2',\n 'notebook_shim        0.2.2',\n 'numpy                1.19.5',\n 'packaging            23.0',\n 'pandas               1.3.2',\n 'pandocfilters        1.5.0',\n 'parso                0.8.3',\n 'patsy                0.5.1',\n 'pexpect              4.8.0',\n 'pickleshare          0.7.5',\n 'Pillow               9.4.0',\n 'pip                  22.3.1',\n 'pkgutil_resolve_name 1.3.10',\n 'plotly               5.11.0',\n 'pluggy               0.13.1',\n 'pmdarima             1.8.2',\n 'prometheus-client    0.15.0',\n 'prompt-toolkit       3.0.36',\n 'ptvsd                4.3.2',\n 'ptyprocess           0.7.0',\n 'py                   1.11.0',\n 'pyarrow              5.0.0',\n 'pycparser            2.21',\n 'pydevd               2.5.0',\n 'Pygments             2.14.0',\n 'pyhocon              0.3.58',\n 'PyMySQL              1.0.2',\n 'pyparsing            2.4.7',\n 'pyrsistent           0.19.3',\n 'pyspnego             0.7.0',\n 'pytest               6.2.3',\n 'python-dateutil      2.8.2',\n 'pytz                 2021.1',\n 'PyYAML               5.4.1',\n 'pyzmq                24.0.1',\n 'qtconsole            5.4.0',\n 'QtPy                 2.3.0',\n 'regex                2022.10.31',\n 'requests             2.23.0',\n 'requests-kerberos    0.14.0',\n 's3fs                 2021.8.1',\n 's3transfer           0.5.0',\n 'scikit-learn         0.24.2',\n 'scipy                1.7.1',\n 'Send2Trash           1.8.0',\n 'setuptools           49.1.3',\n 'six                  1.16.0',\n 'sniffio              1.3.0',\n 'soupsieve            2.3.2.post1',\n 'sparkmagic           0.18.0',\n 'statsmodels          0.12.2',\n 'subprocess32         3.5.4',\n 'sympy                1.8',\n 'tbats                1.1.0',\n 'tenacity             8.1.0',\n 'terminado            0.17.1',\n 'threadpoolctl        3.1.0',\n 'tinycss2             1.2.1',\n 'toml                 0.10.2',\n 'tomlkit              0.11.6',\n 'tornado              6.2',\n 'tqdm                 4.64.1',\n 'traitlets            5.8.1',\n 'typing_extensions    4.4.0',\n 'urllib3              1.25.11',\n 'wcwidth              0.2.5',\n 'webencodings         0.5.1',\n 'websocket-client     1.4.2',\n 'wheel                0.37.0',\n 'widgetsnbextension   3.5.2',\n 'wrapt                1.14.1',\n 'yarl                 1.8.2',\n 'zipp                 3.11.0',\n '',\n '[notice] A new release of pip available: 22.3.1 -> 23.1.1',\n '[notice] To update, run: python3 -m pip install --upgrade pip']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%!\n",
    "pwd\n",
    "ls -la\n",
    "pip list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T15:01:50.871565Z",
     "end_time": "2023-04-22T15:01:52.472584Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = spark.read.json(\"../assets/sample.json\")\n",
    "\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T20:37:07.648704Z",
     "end_time": "2023-04-21T20:37:07.898317Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o80.json.\n: org.apache.hadoop.fs.s3a.AWSBadRequestException: doesBucketExist on oleksandr-glue-job-src: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 6DQHM845FJKZXAYK; S3 Extended Request ID: Fr4qxnUI1rhcxJWBJiXaEgMShoPTAG1604oYwBWdEX18eOFYFNaf4OCnaSVyAfwKWjUMdjl/5gYMfYG1eEwlhw==; Proxy: null), S3 Extended Request ID: Fr4qxnUI1rhcxJWBJiXaEgMShoPTAG1604oYwBWdEX18eOFYFNaf4OCnaSVyAfwKWjUMdjl/5gYMfYG1eEwlhw==:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 6DQHM845FJKZXAYK; S3 Extended Request ID: Fr4qxnUI1rhcxJWBJiXaEgMShoPTAG1604oYwBWdEX18eOFYFNaf4OCnaSVyAfwKWjUMdjl/5gYMfYG1eEwlhw==; Proxy: null)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:224)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3358)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:520)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 6DQHM845FJKZXAYK; S3 Extended Request ID: Fr4qxnUI1rhcxJWBJiXaEgMShoPTAG1604oYwBWdEX18eOFYFNaf4OCnaSVyAfwKWjUMdjl/5gYMfYG1eEwlhw==; Proxy: null), S3 Extended Request ID: Fr4qxnUI1rhcxJWBJiXaEgMShoPTAG1604oYwBWdEX18eOFYFNaf4OCnaSVyAfwKWjUMdjl/5gYMfYG1eEwlhw==\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5267)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5214)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1439)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1375)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)\n\t... 30 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-409221959a07>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"multiline\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"true\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"s3://oleksandr-glue-job-src/open_food_facts.json\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mjson\u001B[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers, modifiedBefore, modifiedAfter)\u001B[0m\n\u001B[1;32m    370\u001B[0m             \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    371\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 372\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    373\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    374\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1304\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1305\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1307\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m                 raise Py4JError(\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o80.json.\n: org.apache.hadoop.fs.s3a.AWSBadRequestException: doesBucketExist on oleksandr-glue-job-src: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 6DQHM845FJKZXAYK; S3 Extended Request ID: Fr4qxnUI1rhcxJWBJiXaEgMShoPTAG1604oYwBWdEX18eOFYFNaf4OCnaSVyAfwKWjUMdjl/5gYMfYG1eEwlhw==; Proxy: null), S3 Extended Request ID: Fr4qxnUI1rhcxJWBJiXaEgMShoPTAG1604oYwBWdEX18eOFYFNaf4OCnaSVyAfwKWjUMdjl/5gYMfYG1eEwlhw==:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 6DQHM845FJKZXAYK; S3 Extended Request ID: Fr4qxnUI1rhcxJWBJiXaEgMShoPTAG1604oYwBWdEX18eOFYFNaf4OCnaSVyAfwKWjUMdjl/5gYMfYG1eEwlhw==; Proxy: null)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:224)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:380)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:314)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3358)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:520)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 6DQHM845FJKZXAYK; S3 Extended Request ID: Fr4qxnUI1rhcxJWBJiXaEgMShoPTAG1604oYwBWdEX18eOFYFNaf4OCnaSVyAfwKWjUMdjl/5gYMfYG1eEwlhw==; Proxy: null), S3 Extended Request ID: Fr4qxnUI1rhcxJWBJiXaEgMShoPTAG1604oYwBWdEX18eOFYFNaf4OCnaSVyAfwKWjUMdjl/5gYMfYG1eEwlhw==\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5267)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5214)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1439)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1375)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:381)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)\n\t... 30 more\n"
     ]
    }
   ],
   "source": [
    "# multiline parameter is necessary to make Spark read not structured data that might contain different columns across rows\n",
    "df = spark.read.option(\"multiline\", \"true\").json(\"PATH_TO_JSON_FILE_IN_S3\")\n",
    "\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
